{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, TypeVar, Union\n",
    "\n",
    "import math\n",
    "import numpy\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "        \n",
    "    batch_size = sequence_length.size(0)\n",
    "    \n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6333, 0.8669],\n",
      "         [0.1677, 0.7046],\n",
      "         [0.6290, 0.3936]],\n",
      "\n",
      "        [[0.6042, 0.6700],\n",
      "         [0.7108, 0.1940],\n",
      "         [0.5285, 0.3843]],\n",
      "\n",
      "        [[0.2732, 0.4841],\n",
      "         [0.2949, 0.1905],\n",
      "         [0.7884, 0.4613]],\n",
      "\n",
      "        [[0.5274, 0.4262],\n",
      "         [0.3142, 0.2303],\n",
      "         [0.5850, 0.4656]]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# logits: (batch, seq_len, num_classes)\n",
    "# target: (batch, seq_len)\n",
    "# length: (batch,)\n",
    "batch_size = 4\n",
    "nb_classes = 2\n",
    "seq_len = 3\n",
    "\n",
    "logits = torch.rand(batch_size, seq_len, nb_classes)\n",
    "target = torch.rand(batch_size, seq_len)\n",
    "target = target.long()\n",
    "\n",
    "print(logits)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = ([[1],\n",
    "        [2],\n",
    "        [3],\n",
    "        [4]])\n",
    "\n",
    "length = torch.LongTensor(4)\n",
    "length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = compute_loss(logits, target, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6333, 0.8669],\n",
      "        [0.1677, 0.7046],\n",
      "        [0.6290, 0.3936],\n",
      "        [0.6042, 0.6700],\n",
      "        [0.7108, 0.1940],\n",
      "        [0.5285, 0.3843],\n",
      "        [0.2732, 0.4841],\n",
      "        [0.2949, 0.1905],\n",
      "        [0.7884, 0.4613],\n",
      "        [0.5274, 0.4262],\n",
      "        [0.3142, 0.2303],\n",
      "        [0.5850, 0.4656]])\n",
      "tensor([[-0.8168, -0.5831],\n",
      "        [-0.9972, -0.4603],\n",
      "        [-0.5824, -0.8177],\n",
      "        [-0.7266, -0.6608],\n",
      "        [-0.4678, -0.9846],\n",
      "        [-0.6236, -0.7679],\n",
      "        [-0.8041, -0.5933],\n",
      "        [-0.6423, -0.7467],\n",
      "        [-0.5429, -0.8700],\n",
      "        [-0.6438, -0.7450],\n",
      "        [-0.6521, -0.7360],\n",
      "        [-0.6352, -0.7547]])\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "tensor([[0.8168],\n",
      "        [0.9972],\n",
      "        [0.5824],\n",
      "        [0.7266],\n",
      "        [0.4678],\n",
      "        [0.6236],\n",
      "        [0.8041],\n",
      "        [0.6423],\n",
      "        [0.5429],\n",
      "        [0.6438],\n",
      "        [0.6521],\n",
      "        [0.6352]])\n",
      "tensor([[0.8168, 0.9972, 0.5824],\n",
      "        [0.7266, 0.4678, 0.6236],\n",
      "        [0.8041, 0.6423, 0.5429],\n",
      "        [0.6438, 0.6521, 0.6352]])\n"
     ]
    }
   ],
   "source": [
    "# logits_flat: (batch * max_len, num_classes)\n",
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "print(logits_flat)\n",
    "\n",
    "# log_probs_flat: (batch * max_len, num_classes)\n",
    "log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n",
    "print(log_probs_flat)\n",
    "\n",
    "# target_flat: (batch * max_len, 1)\n",
    "target_flat = target.view(-1, 1)\n",
    "print(target_flat)\n",
    "\n",
    "# losses_flat: (batch * max_len, 1)\n",
    "# dimensions need to be the same size except for the dim you are gathering on\n",
    "losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "print(losses_flat)\n",
    "\n",
    "# losses: (batch, max_len)\n",
    "losses = losses_flat.view(*target.size())\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.uint8)\n",
      "tensor([[0.8168, 0.9972, 0.5824],\n",
      "        [0.7266, 0.4678, 0.6236],\n",
      "        [0.8041, 0.6423, 0.5429],\n",
      "        [0.6438, 0.6521, 0.6352]])\n",
      "tensor(9.3689e-19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# mask: (batch, max_len)\n",
    "mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "print(mask)\n",
    "\n",
    "losses = losses * mask.float()\n",
    "print(losses)\n",
    "\n",
    "loss = losses.sum() / length.float().sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 2 - 3d target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6469, 0.4497, 0.5300, 0.8275, 0.4541, 0.9426, 0.2814, 0.0265,\n",
      "          0.5041, 0.6166],\n",
      "         [0.4891, 0.1579, 0.0265, 0.2191, 0.5626, 0.9797, 0.9507, 0.5394,\n",
      "          0.0937, 0.3389],\n",
      "         [0.3268, 0.8648, 0.3931, 0.0846, 0.3540, 0.7184, 0.7297, 0.9259,\n",
      "          0.7639, 0.3943]],\n",
      "\n",
      "        [[0.7751, 0.0392, 0.2076, 0.2525, 0.1707, 0.5629, 0.8782, 0.4542,\n",
      "          0.8991, 0.4684],\n",
      "         [0.7847, 0.8519, 0.6794, 0.6619, 0.4287, 0.0022, 0.5121, 0.9886,\n",
      "          0.5650, 0.0018],\n",
      "         [0.4407, 0.5124, 0.4109, 0.7643, 0.5718, 0.9197, 0.3073, 0.7319,\n",
      "          0.0625, 0.1921]],\n",
      "\n",
      "        [[0.7653, 0.2033, 0.9746, 0.5036, 0.0085, 0.2511, 0.3767, 0.2880,\n",
      "          0.1173, 0.8549],\n",
      "         [0.5013, 0.3789, 0.6818, 0.6572, 0.8783, 0.2324, 0.2101, 0.5186,\n",
      "          0.9100, 0.6705],\n",
      "         [0.7169, 0.8573, 0.0490, 0.7078, 0.7896, 0.6839, 0.2321, 0.6763,\n",
      "          0.7682, 0.2440]],\n",
      "\n",
      "        [[0.8551, 0.8265, 0.5504, 0.6530, 0.9327, 0.4980, 0.7012, 0.9977,\n",
      "          0.8583, 0.9990],\n",
      "         [0.7837, 0.1393, 0.1600, 0.1031, 0.1880, 0.3629, 0.6445, 0.1489,\n",
      "          0.9721, 0.7878],\n",
      "         [0.7963, 0.3091, 0.1214, 0.5966, 0.3641, 0.1044, 0.3576, 0.5776,\n",
      "          0.2907, 0.1392]]])\n",
      "tensor([[[0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0]],\n",
      "\n",
      "        [[0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0]],\n",
      "\n",
      "        [[0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0]],\n",
      "\n",
      "        [[0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "# logits: (batch, seq_len, num_classes)\n",
    "# target: (batch, seq_len)\n",
    "# length: (batch,)\n",
    "batch_size = 4\n",
    "nb_classes = 10\n",
    "nb_labels = 2\n",
    "seq_len = 3\n",
    "\n",
    "logits = torch.rand(batch_size, seq_len, nb_classes)\n",
    "target = torch.rand(batch_size, seq_len, nb_labels)\n",
    "target = target.long()\n",
    "\n",
    "print(logits)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = ([[1],\n",
    "        [2],\n",
    "        [3],\n",
    "        [4]])\n",
    "\n",
    "length = torch.LongTensor(4)\n",
    "length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = ([[[1, 0],\n",
    "         [1, 2],\n",
    "         [1, 3]],\n",
    "\n",
    "        [[2, 2],\n",
    "         [5, 4],\n",
    "         [6, 8]],\n",
    "\n",
    "        [[3, 9],\n",
    "         [4, 6],\n",
    "         [6, 3]],\n",
    "\n",
    "        [[4, 6],\n",
    "         [2, 3],\n",
    "         [7, 9]]])\n",
    "target = torch.LongTensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== log probs flat == \n",
      " tensor([[-2.2134, -2.4105, -2.3302, -2.0327, -2.4061, -1.9176, -2.5788, -2.8337,\n",
      "         -2.3561, -2.2436],\n",
      "        [-2.3019, -2.6331, -2.7645, -2.5719, -2.2284, -1.8113, -1.8402, -2.2516,\n",
      "         -2.6972, -2.4521],\n",
      "        [-2.5656, -2.0275, -2.4992, -2.8078, -2.5383, -2.1739, -2.1627, -1.9664,\n",
      "         -2.1284, -2.4980],\n",
      "        [-2.0407, -2.7767, -2.6082, -2.5633, -2.6451, -2.2529, -1.9376, -2.3617,\n",
      "         -1.9167, -2.3474],\n",
      "        [-2.1113, -2.0441, -2.2166, -2.2341, -2.4673, -2.8939, -2.3840, -1.9074,\n",
      "         -2.3310, -2.8943],\n",
      "        [-2.3851, -2.3134, -2.4148, -2.0615, -2.2540, -1.9060, -2.5184, -2.0939,\n",
      "         -2.7632, -2.6337],\n",
      "        [-2.0222, -2.5843, -1.8129, -2.2839, -2.7790, -2.5364, -2.4108, -2.4996,\n",
      "         -2.6702, -1.9326],\n",
      "        [-2.3912, -2.5137, -2.2108, -2.2354, -2.0143, -2.6602, -2.6825, -2.3740,\n",
      "         -1.9825, -2.2221],\n",
      "        [-2.1916, -2.0512, -2.8594, -2.2007, -2.1188, -2.2246, -2.6764, -2.2321,\n",
      "         -2.1403, -2.6645],\n",
      "        [-2.2487, -2.2772, -2.5534, -2.4508, -2.1710, -2.6058, -2.4026, -2.1061,\n",
      "         -2.2455, -2.1048],\n",
      "        [-1.9997, -2.6440, -2.6233, -2.6803, -2.5954, -2.4204, -2.1389, -2.6345,\n",
      "         -1.8112, -1.9955],\n",
      "        [-1.8965, -2.3837, -2.5713, -2.0962, -2.3287, -2.5883, -2.3351, -2.1152,\n",
      "         -2.4020, -2.5535]])\n",
      "== target flat == \n",
      " tensor([[1, 0],\n",
      "        [1, 2],\n",
      "        [1, 3],\n",
      "        [2, 2],\n",
      "        [5, 4],\n",
      "        [6, 8],\n",
      "        [3, 9],\n",
      "        [4, 6],\n",
      "        [6, 3],\n",
      "        [4, 6],\n",
      "        [2, 3],\n",
      "        [7, 9]])\n",
      "== losses flat == \n",
      " tensor([[2.4105, 2.2134],\n",
      "        [2.6331, 2.7645],\n",
      "        [2.0275, 2.8078],\n",
      "        [2.6082, 2.6082],\n",
      "        [2.8939, 2.4673],\n",
      "        [2.5184, 2.7632],\n",
      "        [2.2839, 1.9326],\n",
      "        [2.0143, 2.6825],\n",
      "        [2.6764, 2.2007],\n",
      "        [2.1710, 2.4026],\n",
      "        [2.6233, 2.6803],\n",
      "        [2.1152, 2.5535]])\n",
      "== losses == \n",
      " tensor([[[2.4105, 2.2134],\n",
      "         [2.6331, 2.7645],\n",
      "         [2.0275, 2.8078]],\n",
      "\n",
      "        [[2.6082, 2.6082],\n",
      "         [2.8939, 2.4673],\n",
      "         [2.5184, 2.7632]],\n",
      "\n",
      "        [[2.2839, 1.9326],\n",
      "         [2.0143, 2.6825],\n",
      "         [2.6764, 2.2007]],\n",
      "\n",
      "        [[2.1710, 2.4026],\n",
      "         [2.6233, 2.6803],\n",
      "         [2.1152, 2.5535]]])\n"
     ]
    }
   ],
   "source": [
    "# logits_flat: (batch * max_len, num_classes)\n",
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "#print(logits_flat)\n",
    "\n",
    "# log_probs_flat: (batch * max_len, num_classes)\n",
    "log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n",
    "print(\"== log probs flat == \\n\", log_probs_flat)\n",
    "\n",
    "# target_flat: (batch * max_len, 1)\n",
    "#target_flat = target.view(-1, 1)\n",
    "target_flat = target.view(-1, nb_labels)\n",
    "print(\"== target flat == \\n\", target_flat)\n",
    "\n",
    "# losses_flat: (batch * max_len, 1)\n",
    "# dimensions need to be the same size except for the dim you are gathering on\n",
    "# gathers from log probs the target index (row dim)\n",
    "losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "print(\"== losses flat == \\n\", losses_flat)\n",
    "\n",
    "# losses: (batch, max_len)\n",
    "losses = losses_flat.view(*target.size())\n",
    "print(\"== losses == \\n\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]])\n"
     ]
    }
   ],
   "source": [
    "ymask = losses_flat.data.new(losses_flat.size()).zero_() # (all zero)\n",
    "ymask = ymask.long()\n",
    "\n",
    "print(ymask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zmask = ymask + target\n",
    "# zmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 7],\n",
       "        [7, 1],\n",
       "        [7, 7],\n",
       "        [7, 7],\n",
       "        [7, 7],\n",
       "        [7, 0],\n",
       "        [7, 7],\n",
       "        [7, 0],\n",
       "        [0, 7],\n",
       "        [0, 7],\n",
       "        [0, 0],\n",
       "        [0, 0]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ymask.scatter_(0, target_flat, 7) # .view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
