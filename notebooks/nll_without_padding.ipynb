{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, TypeVar, Union\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "        \n",
    "    batch_size = sequence_length.size(0)\n",
    "    \n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits: (batch, seq_len, num_classes)\n",
    "# target: (batch, seq_len)\n",
    "\n",
    "batch_size = 4\n",
    "nb_classes = 2\n",
    "seq_len = 3\n",
    "\n",
    "logits = torch.rand(batch_size, seq_len, nb_classes)\n",
    "target = torch.rand(batch_size, seq_len)\n",
    "target = target.long()\n",
    "\n",
    "print(logits)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = ([[1],\n",
    "        [2],\n",
    "        [3],\n",
    "        [4]])\n",
    "\n",
    "length = torch.LongTensor(4)\n",
    "length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = compute_loss(logits, target, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits_flat: (batch * max_len, num_classes)\n",
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "print(logits_flat)\n",
    "\n",
    "# log_probs_flat: (batch * max_len, num_classes)\n",
    "log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n",
    "print(log_probs_flat)\n",
    "\n",
    "# target_flat: (batch * max_len, 1)\n",
    "target_flat = target.view(-1, 1)\n",
    "print(target_flat)\n",
    "\n",
    "# losses_flat: (batch * max_len, 1)\n",
    "# dimensions need to be the same size except for the dim you are gathering on\n",
    "losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "print(losses_flat)\n",
    "\n",
    "# losses: (batch, max_len)\n",
    "losses = losses_flat.view(*target.size())\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask: (batch, max_len)\n",
    "mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "print(mask)\n",
    "\n",
    "losses = losses * mask.float()\n",
    "print(losses)\n",
    "\n",
    "loss = losses.sum() / length.float().sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 2 - 3d target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 10])\n",
      "torch.Size([4, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# logits: (batch, seq_len, num_classes)\n",
    "# target: (batch, seq_len)\n",
    "# length: (batch,)\n",
    "batch_size = 4\n",
    "nb_classes = 10\n",
    "nb_labels = 3\n",
    "seq_len = 5\n",
    "\n",
    "logits = torch.rand(batch_size, seq_len, nb_classes)\n",
    "# notice target has a third dimension of nb_labels\n",
    "target = torch.rand(batch_size, seq_len, nb_labels)\n",
    "target = target.long()\n",
    "\n",
    "print(logits.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# create a target tensor with some padded elements\n",
    "target = ([[[1, 0, 0],\n",
    "         [1, 2, 0],\n",
    "         [1, 4, 0],\n",
    "         [1, 3, 0],\n",
    "         [1, 3, 0]],\n",
    "\n",
    "        [[2, 2, 0],\n",
    "         [5, 4, 0],\n",
    "         [1, 8, 0],\n",
    "         [3, 1, 0],\n",
    "         [6, 8, 0]],\n",
    "\n",
    "        [[3, 9, 0],\n",
    "         [4, 6, 0],\n",
    "         [4, 3, 0],\n",
    "         [3, 1, 0],\n",
    "         [6, 3, 0]],\n",
    "\n",
    "        [[4, 6, 0],\n",
    "         [2, 3, 3],\n",
    "         [1, 2, 9],\n",
    "         [1, 2, 6],\n",
    "         [7, 9, 8]]])\n",
    "\n",
    "target = torch.LongTensor(target)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== log probs flat == \n",
      " tensor([[-1.8218, -2.7636, -1.9159, -2.1004, -2.6394, -2.0386, -2.6950, -2.8105,\n",
      "         -2.4041, -2.4545],\n",
      "        [-2.5887, -2.2012, -2.4453, -1.9583, -2.5401, -2.7374, -1.9999, -2.0180,\n",
      "         -2.1662, -2.8145],\n",
      "        [-1.9865, -2.4331, -2.4818, -1.9130, -2.3507, -2.5870, -2.3594, -2.0848,\n",
      "         -2.5818, -2.5406],\n",
      "        [-2.5858, -2.6240, -1.9591, -2.5773, -2.2977, -2.3896, -1.9731, -1.8685,\n",
      "         -2.5250, -2.6653],\n",
      "        [-2.4359, -2.2461, -2.4838, -2.6442, -2.4852, -2.5410, -2.4728, -1.7417,\n",
      "         -1.9124, -2.4914],\n",
      "        [-2.2621, -2.5824, -1.9419, -2.1887, -2.3836, -2.0864, -2.5794, -2.4914,\n",
      "         -2.0394, -2.8182],\n",
      "        [-2.7964, -2.0027, -1.8743, -2.1708, -2.3545, -2.3196, -2.4992, -2.8081,\n",
      "         -2.4870, -2.1406],\n",
      "        [-2.2576, -2.3424, -2.3087, -2.0824, -2.5972, -2.2311, -2.5954, -2.3671,\n",
      "         -2.0642, -2.3219],\n",
      "        [-2.8438, -2.0766, -2.4358, -2.4724, -1.9623, -2.0347, -2.7247, -2.5042,\n",
      "         -2.1648, -2.1964],\n",
      "        [-2.0492, -2.3373, -1.9760, -2.8125, -2.7938, -2.2366, -2.7004, -2.5582,\n",
      "         -2.2649, -1.8362],\n",
      "        [-1.9105, -2.5558, -2.0123, -2.7444, -2.5162, -2.3207, -2.8031, -2.5884,\n",
      "         -1.8550, -2.2519],\n",
      "        [-2.3971, -2.6507, -1.8673, -2.4358, -2.2093, -2.4713, -2.1888, -2.1985,\n",
      "         -2.4407, -2.3863],\n",
      "        [-2.4166, -2.5142, -2.0544, -2.3211, -2.6666, -1.9292, -2.3993, -2.6124,\n",
      "         -1.8281, -2.7502],\n",
      "        [-2.2061, -2.1517, -2.3283, -2.1925, -2.4340, -2.6260, -2.1297, -2.0507,\n",
      "         -2.4255, -2.6814],\n",
      "        [-2.1221, -2.1544, -1.9444, -2.5252, -2.7710, -2.0114, -2.6973, -2.0060,\n",
      "         -2.5426, -2.7464],\n",
      "        [-2.3428, -2.3954, -2.0059, -2.4828, -1.8369, -2.4918, -2.1629, -2.2575,\n",
      "         -2.7453, -2.6756],\n",
      "        [-2.2922, -2.0782, -2.5147, -2.0703, -2.8589, -2.2253, -2.6277, -2.4259,\n",
      "         -1.9744, -2.2837],\n",
      "        [-2.6874, -2.8802, -2.0709, -2.1576, -2.3985, -1.9789, -2.7064, -2.6073,\n",
      "         -2.0274, -2.0184],\n",
      "        [-2.6850, -2.2149, -2.3970, -2.0833, -1.9606, -2.0734, -2.3122, -2.3347,\n",
      "         -2.7696, -2.5010],\n",
      "        [-2.5684, -2.4669, -2.0107, -1.9200, -2.3094, -2.2322, -2.6102, -2.8489,\n",
      "         -1.9584, -2.5399]])\n",
      "== target flat == \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 2, 0],\n",
      "        [1, 4, 0],\n",
      "        [1, 3, 0],\n",
      "        [1, 3, 0],\n",
      "        [2, 2, 0],\n",
      "        [5, 4, 0],\n",
      "        [1, 8, 0],\n",
      "        [3, 1, 0],\n",
      "        [6, 8, 0],\n",
      "        [3, 9, 0],\n",
      "        [4, 6, 0],\n",
      "        [4, 3, 0],\n",
      "        [3, 1, 0],\n",
      "        [6, 3, 0],\n",
      "        [4, 6, 0],\n",
      "        [2, 3, 3],\n",
      "        [1, 2, 9],\n",
      "        [1, 2, 6],\n",
      "        [7, 9, 8]])\n",
      "== losses flat == \n",
      " tensor([[2.7636, 1.8218, 1.8218],\n",
      "        [2.2012, 2.4453, 2.5887],\n",
      "        [2.4331, 2.3507, 1.9865],\n",
      "        [2.6240, 2.5773, 2.5858],\n",
      "        [2.2461, 2.6442, 2.4359],\n",
      "        [1.9419, 1.9419, 2.2621],\n",
      "        [2.3196, 2.3545, 2.7964],\n",
      "        [2.3424, 2.0642, 2.2576],\n",
      "        [2.4724, 2.0766, 2.8438],\n",
      "        [2.7004, 2.2649, 2.0492],\n",
      "        [2.7444, 2.2519, 1.9105],\n",
      "        [2.2093, 2.1888, 2.3971],\n",
      "        [2.6666, 2.3211, 2.4166],\n",
      "        [2.1925, 2.1517, 2.2061],\n",
      "        [2.6973, 2.5252, 2.1221],\n",
      "        [1.8369, 2.1629, 2.3428],\n",
      "        [2.5147, 2.0703, 2.0703],\n",
      "        [2.8802, 2.0709, 2.0184],\n",
      "        [2.2149, 2.3970, 2.3122],\n",
      "        [2.8489, 2.5399, 1.9584]])\n",
      "== losses == \n",
      " tensor([[[2.7636, 1.8218, 1.8218],\n",
      "         [2.2012, 2.4453, 2.5887],\n",
      "         [2.4331, 2.3507, 1.9865],\n",
      "         [2.6240, 2.5773, 2.5858],\n",
      "         [2.2461, 2.6442, 2.4359]],\n",
      "\n",
      "        [[1.9419, 1.9419, 2.2621],\n",
      "         [2.3196, 2.3545, 2.7964],\n",
      "         [2.3424, 2.0642, 2.2576],\n",
      "         [2.4724, 2.0766, 2.8438],\n",
      "         [2.7004, 2.2649, 2.0492]],\n",
      "\n",
      "        [[2.7444, 2.2519, 1.9105],\n",
      "         [2.2093, 2.1888, 2.3971],\n",
      "         [2.6666, 2.3211, 2.4166],\n",
      "         [2.1925, 2.1517, 2.2061],\n",
      "         [2.6973, 2.5252, 2.1221]],\n",
      "\n",
      "        [[1.8369, 2.1629, 2.3428],\n",
      "         [2.5147, 2.0703, 2.0703],\n",
      "         [2.8802, 2.0709, 2.0184],\n",
      "         [2.2149, 2.3970, 2.3122],\n",
      "         [2.8489, 2.5399, 1.9584]]])\n"
     ]
    }
   ],
   "source": [
    "# logits_flat: (batch * max_len, num_classes)\n",
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "#print(\"== logits flat == \\n\", logits_flat)\n",
    "\n",
    "# log_probs_flat: (batch * max_len, num_classes)\n",
    "log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n",
    "print(\"== log probs flat == \\n\", log_probs_flat)\n",
    "\n",
    "# target_flat: (batch * max_len, 1)\n",
    "#target_flat = target.view(-1, 1)\n",
    "target_flat = target.view(-1, nb_labels)\n",
    "print(\"== target flat == \\n\", target_flat)\n",
    "\n",
    "# losses_flat: (batch * max_len, 1)\n",
    "# dimensions need to be the same size except for the dim you are gathering on\n",
    "# gathers from log probs the target index (row dim)\n",
    "losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "print(\"== losses flat == \\n\", losses_flat)\n",
    "\n",
    "# losses: (batch, max_len)\n",
    "losses = losses_flat.view(*target.size())\n",
    "print(\"== losses == \\n\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "# padding\n",
    "# 2d padding has shape (batch_size, seq_len), need to obtain the last dimension\n",
    "weights = torch.zeros(batch_size, seq_len)\n",
    "print(weights)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch nonzero on last element of targets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(target.data).size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 1 2]\n",
      "  [0 3 4]\n",
      "  [4 1 1]\n",
      "  [5 0 4]\n",
      "  [1 3 4]]\n",
      "\n",
      " [[3 0 2]\n",
      "  [2 2 5]\n",
      "  [3 0 1]\n",
      "  [5 2 4]\n",
      "  [2 0 2]]\n",
      "\n",
      " [[4 2 5]\n",
      "  [2 1 3]\n",
      "  [5 1 5]\n",
      "  [2 3 5]\n",
      "  [4 4 0]]\n",
      "\n",
      " [[5 3 1]\n",
      "  [5 0 3]\n",
      "  [0 5 5]\n",
      "  [3 5 4]\n",
      "  [1 5 4]]]\n",
      "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3]), array([0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 0, 0, 1, 1, 1, 2, 2, 3, 3,\n",
      "       3, 4, 4, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 0, 0, 0, 1, 1,\n",
      "       2, 2, 3, 3, 3, 4, 4, 4]), array([0, 1, 2, 1, 2, 0, 1, 2, 0, 2, 0, 1, 2, 0, 2, 0, 1, 2, 0, 2, 0, 1,\n",
      "       2, 0, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 0, 1, 2, 0, 2,\n",
      "       1, 2, 0, 1, 2, 0, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randint(6,size=(4,5,3))\n",
    "print(a)\n",
    "\n",
    "idx = np.nonzero(a)\n",
    "print(idx)\n",
    "#a[idx] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "ymask = losses_flat.data.new(losses_flat.size()).zero_() # (all zero)\n",
    "ymask = ymask.long()\n",
    "\n",
    "print(ymask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zmask = ymask + target\n",
    "# zmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymask.scatter_(0, target_flat, 7) # .view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
