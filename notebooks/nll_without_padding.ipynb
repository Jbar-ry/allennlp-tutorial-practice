{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, TypeVar, Union\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "        \n",
    "    batch_size = sequence_length.size(0)\n",
    "    \n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits: (batch, seq_len, num_classes)\n",
    "# target: (batch, seq_len)\n",
    "\n",
    "batch_size = 4\n",
    "nb_classes = 2\n",
    "seq_len = 3\n",
    "\n",
    "logits = torch.rand(batch_size, seq_len, nb_classes)\n",
    "target = torch.rand(batch_size, seq_len)\n",
    "target = target.long()\n",
    "\n",
    "print(logits)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logits_flat: (batch * max_len, num_classes)\n",
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "print(logits_flat)\n",
    "\n",
    "# log_probs_flat: (batch * max_len, num_classes)\n",
    "log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n",
    "print(log_probs_flat)\n",
    "\n",
    "# target_flat: (batch * max_len, 1)\n",
    "target_flat = target.view(-1, 1)\n",
    "print(target_flat)\n",
    "\n",
    "# losses_flat: (batch * max_len, 1)\n",
    "# dimensions need to be the same size except for the dim you are gathering on\n",
    "losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "print(losses_flat)\n",
    "\n",
    "# losses: (batch, max_len)\n",
    "losses = losses_flat.view(*target.size())\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mask: (batch, max_len)\n",
    "mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "print(mask)\n",
    "\n",
    "losses = losses * mask.float()\n",
    "print(losses)\n",
    "\n",
    "loss = losses.sum() / length.float().sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 2 - 3d target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 10])\n",
      "torch.Size([4, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# logits: (batch, seq_len, num_classes)\n",
    "# target: (batch, seq_len)\n",
    "# length: (batch,)\n",
    "batch_size = 4\n",
    "nb_classes = 10\n",
    "nb_labels = 3\n",
    "seq_len = 5\n",
    "\n",
    "logits = torch.rand(batch_size, seq_len, nb_classes)\n",
    "# notice target has a third dimension of nb_labels\n",
    "target = torch.rand(batch_size, seq_len, nb_labels)\n",
    "target = target.long()\n",
    "\n",
    "print(logits.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# create a target tensor with some padded elements\n",
    "target = ([[[1, 0, 0],\n",
    "         [1, 2, 0],\n",
    "         [1, 4, 0],\n",
    "         [1, 3, 0],\n",
    "         [1, 3, 0]],\n",
    "\n",
    "        [[2, 2, 0],\n",
    "         [5, 4, 0],\n",
    "         [1, 8, 0],\n",
    "         [3, 1, 0],\n",
    "         [6, 8, 0]],\n",
    "\n",
    "        [[3, 9, 0],\n",
    "         [4, 6, 0],\n",
    "         [4, 3, 0],\n",
    "         [3, 1, 0],\n",
    "         [6, 3, 0]],\n",
    "\n",
    "        [[4, 6, 0],\n",
    "         [2, 3, 3],\n",
    "         [1, 2, 9],\n",
    "         [1, 2, 6],\n",
    "         [7, 9, 8]]])\n",
    "\n",
    "target = torch.LongTensor(target)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 0, 0],\n",
      "         [3, 3, 4],\n",
      "         [4, 5, 1],\n",
      "         [4, 2, 2],\n",
      "         [5, 5, 2]],\n",
      "\n",
      "        [[1, 5, 5],\n",
      "         [4, 1, 2],\n",
      "         [1, 4, 5],\n",
      "         [5, 2, 2],\n",
      "         [3, 4, 4]],\n",
      "\n",
      "        [[5, 4, 0],\n",
      "         [2, 1, 3],\n",
      "         [2, 3, 0],\n",
      "         [1, 0, 1],\n",
      "         [2, 5, 1]],\n",
      "\n",
      "        [[5, 4, 2],\n",
      "         [2, 1, 1],\n",
      "         [3, 2, 0],\n",
      "         [3, 4, 2],\n",
      "         [0, 1, 4]]])\n"
     ]
    }
   ],
   "source": [
    "target = torch.randint(6, size=(4,5,3))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== log probs flat == \n",
      " tensor([[-2.5541, -2.6483, -1.9175, -2.3984, -1.9399, -2.5420, -2.1021, -2.6512,\n",
      "         -1.9804, -2.8061],\n",
      "        [-2.3979, -2.3752, -2.3320, -2.3054, -2.0664, -2.2317, -2.7316, -1.9382,\n",
      "         -2.2414, -2.6605],\n",
      "        [-2.2840, -2.2207, -1.8146, -2.6244, -2.6774, -2.7192, -1.8871, -2.0133,\n",
      "         -2.7447, -2.6564],\n",
      "        [-2.6910, -1.9292, -2.2400, -2.8074, -2.0766, -2.3287, -2.3621, -2.3430,\n",
      "         -2.8001, -1.9223],\n",
      "        [-2.5979, -2.2199, -2.5472, -2.8370, -1.9948, -2.1050, -2.1352, -1.9835,\n",
      "         -2.9466, -2.1731],\n",
      "        [-2.7588, -2.6624, -1.9396, -2.5732, -2.3752, -2.6498, -2.6546, -2.0218,\n",
      "         -1.9942, -1.9360],\n",
      "        [-2.0923, -2.7621, -2.3564, -2.4340, -1.9870, -2.7169, -1.9742, -2.6905,\n",
      "         -2.0041, -2.4499],\n",
      "        [-2.4599, -2.6612, -2.3287, -1.9371, -2.4446, -2.5606, -2.4221, -2.0665,\n",
      "         -1.8560, -2.6960],\n",
      "        [-2.0872, -1.9946, -2.0831, -2.3067, -2.2554, -2.6125, -2.8531, -2.2836,\n",
      "         -2.2313, -2.6508],\n",
      "        [-2.2067, -2.5613, -2.1513, -2.0581, -1.9493, -2.1231, -2.7642, -2.2496,\n",
      "         -2.7793, -2.5754],\n",
      "        [-1.8570, -2.2327, -2.4896, -2.5293, -2.3315, -2.4236, -2.6045, -2.5321,\n",
      "         -2.1589, -2.1258],\n",
      "        [-3.0218, -2.1235, -2.2571, -2.3943, -2.2102, -2.0791, -2.1894, -2.1359,\n",
      "         -2.1504, -2.9096],\n",
      "        [-1.9260, -2.2631, -2.8639, -2.4257, -2.3373, -2.2643, -2.2580, -2.4669,\n",
      "         -2.3839, -2.0982],\n",
      "        [-2.6545, -2.1213, -2.2793, -1.9461, -2.6279, -2.1248, -2.3834, -2.5672,\n",
      "         -1.9890, -2.7017],\n",
      "        [-2.0221, -2.0028, -2.2974, -2.4594, -2.3201, -2.5731, -2.1669, -2.4272,\n",
      "         -2.3687, -2.5811],\n",
      "        [-2.1023, -2.6103, -2.6361, -1.8511, -2.5186, -2.6323, -2.6596, -2.3757,\n",
      "         -2.0817, -1.9990],\n",
      "        [-2.0191, -2.4291, -2.7851, -2.7642, -1.8863, -2.4988, -2.3360, -1.9696,\n",
      "         -2.2154, -2.5876],\n",
      "        [-2.4057, -2.1292, -1.9217, -1.9899, -2.6408, -2.5041, -2.8950, -2.8343,\n",
      "         -1.9504, -2.3180],\n",
      "        [-2.1155, -2.3324, -2.5879, -2.6419, -2.0817, -2.4177, -2.2267, -2.3484,\n",
      "         -2.4870, -1.9985],\n",
      "        [-2.1270, -2.6173, -2.4300, -2.2127, -2.4899, -2.5742, -2.1959, -2.5022,\n",
      "         -1.9143, -2.2017]])\n",
      "== target flat == \n",
      " tensor([[1, 0, 0],\n",
      "        [3, 3, 4],\n",
      "        [4, 5, 1],\n",
      "        [4, 2, 2],\n",
      "        [5, 5, 2],\n",
      "        [1, 5, 5],\n",
      "        [4, 1, 2],\n",
      "        [1, 4, 5],\n",
      "        [5, 2, 2],\n",
      "        [3, 4, 4],\n",
      "        [5, 4, 0],\n",
      "        [2, 1, 3],\n",
      "        [2, 3, 0],\n",
      "        [1, 0, 1],\n",
      "        [2, 5, 1],\n",
      "        [5, 4, 2],\n",
      "        [2, 1, 1],\n",
      "        [3, 2, 0],\n",
      "        [3, 4, 2],\n",
      "        [0, 1, 4]])\n",
      "== losses flat == \n",
      " tensor([[2.6483, 2.5541, 2.5541],\n",
      "        [2.3054, 2.3054, 2.0664],\n",
      "        [2.6774, 2.7192, 2.2207],\n",
      "        [2.0766, 2.2400, 2.2400],\n",
      "        [2.1050, 2.1050, 2.5472],\n",
      "        [2.6624, 2.6498, 2.6498],\n",
      "        [1.9870, 2.7621, 2.3564],\n",
      "        [2.6612, 2.4446, 2.5606],\n",
      "        [2.6125, 2.0831, 2.0831],\n",
      "        [2.0581, 1.9493, 1.9493],\n",
      "        [2.4236, 2.3315, 1.8570],\n",
      "        [2.2571, 2.1235, 2.3943],\n",
      "        [2.8639, 2.4257, 1.9260],\n",
      "        [2.1213, 2.6545, 2.1213],\n",
      "        [2.2974, 2.5731, 2.0028],\n",
      "        [2.6323, 2.5186, 2.6361],\n",
      "        [2.7851, 2.4291, 2.4291],\n",
      "        [1.9899, 1.9217, 2.4057],\n",
      "        [2.6419, 2.0817, 2.5879],\n",
      "        [2.1270, 2.6173, 2.4899]])\n",
      "== losses == \n",
      " tensor([[[2.6483, 2.5541, 2.5541],\n",
      "         [2.3054, 2.3054, 2.0664],\n",
      "         [2.6774, 2.7192, 2.2207],\n",
      "         [2.0766, 2.2400, 2.2400],\n",
      "         [2.1050, 2.1050, 2.5472]],\n",
      "\n",
      "        [[2.6624, 2.6498, 2.6498],\n",
      "         [1.9870, 2.7621, 2.3564],\n",
      "         [2.6612, 2.4446, 2.5606],\n",
      "         [2.6125, 2.0831, 2.0831],\n",
      "         [2.0581, 1.9493, 1.9493]],\n",
      "\n",
      "        [[2.4236, 2.3315, 1.8570],\n",
      "         [2.2571, 2.1235, 2.3943],\n",
      "         [2.8639, 2.4257, 1.9260],\n",
      "         [2.1213, 2.6545, 2.1213],\n",
      "         [2.2974, 2.5731, 2.0028]],\n",
      "\n",
      "        [[2.6323, 2.5186, 2.6361],\n",
      "         [2.7851, 2.4291, 2.4291],\n",
      "         [1.9899, 1.9217, 2.4057],\n",
      "         [2.6419, 2.0817, 2.5879],\n",
      "         [2.1270, 2.6173, 2.4899]]])\n"
     ]
    }
   ],
   "source": [
    "# logits_flat: (batch * max_len, num_classes)\n",
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "#print(\"== logits flat == \\n\", logits_flat)\n",
    "\n",
    "# log_probs_flat: (batch * max_len, num_classes)\n",
    "log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n",
    "print(\"== log probs flat == \\n\", log_probs_flat)\n",
    "\n",
    "# target_flat: (batch * max_len, 1)\n",
    "#target_flat = target.view(-1, 1)\n",
    "target_flat = target.view(-1, nb_labels)\n",
    "print(\"== target flat == \\n\", target_flat)\n",
    "\n",
    "# losses_flat: (batch * max_len, 1)\n",
    "# dimensions need to be the same size except for the dim you are gathering on\n",
    "# gathers from log probs the target index (row dim)\n",
    "losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "print(\"== losses flat == \\n\", losses_flat)\n",
    "\n",
    "# losses: (batch, max_len)\n",
    "losses = losses_flat.view(*target.size())\n",
    "print(\"== losses == \\n\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 0.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 0.],\n",
      "         [1., 0., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 0.],\n",
      "         [1., 1., 1.],\n",
      "         [0., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# = torch.LongTensor(a)\n",
    "\n",
    "def mask_tensor(tensor):\n",
    "    return (tensor != 0).long()\n",
    "\n",
    "tensor_mask = mask_tensor(target).float()\n",
    "print(tensor_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.6483, 0.0000, 0.0000],\n",
      "         [2.3054, 2.3054, 2.0664],\n",
      "         [2.6774, 2.7192, 2.2207],\n",
      "         [2.0766, 2.2400, 2.2400],\n",
      "         [2.1050, 2.1050, 2.5472]],\n",
      "\n",
      "        [[2.6624, 2.6498, 2.6498],\n",
      "         [1.9870, 2.7621, 2.3564],\n",
      "         [2.6612, 2.4446, 2.5606],\n",
      "         [2.6125, 2.0831, 2.0831],\n",
      "         [2.0581, 1.9493, 1.9493]],\n",
      "\n",
      "        [[2.4236, 2.3315, 0.0000],\n",
      "         [2.2571, 2.1235, 2.3943],\n",
      "         [2.8639, 2.4257, 0.0000],\n",
      "         [2.1213, 0.0000, 2.1213],\n",
      "         [2.2974, 2.5731, 2.0028]],\n",
      "\n",
      "        [[2.6323, 2.5186, 2.6361],\n",
      "         [2.7851, 2.4291, 2.4291],\n",
      "         [1.9899, 1.9217, 0.0000],\n",
      "         [2.6419, 2.0817, 2.5879],\n",
      "         [0.0000, 2.6173, 2.4899]]])\n"
     ]
    }
   ],
   "source": [
    "# multiply losses by mask\n",
    "masked_losses = losses * tensor_mask\n",
    "print(masked_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now the losses are being multiplied by the mask, zeroing out the loss of the padded labels\n",
    "we need to sum along the row-dim so for each item in the sequence we take its summed loss\n",
    "this can probably be a 2d tensor now of (batch, seq_len) as the seq_len dimension will contain a scalar loss value,\n",
    "then we can average all these losses over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
