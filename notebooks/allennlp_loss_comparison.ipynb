{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, TypeVar, Union\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs to the function\n",
    "\n",
    "# logits shape torch.Size([4, 16, 16])\n",
    "# targets shape torch.Size([4, 16])\n",
    "# weights shape torch.Size([4, 16])\n",
    "\n",
    "\n",
    "# weights tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
    "#         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
    "#         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
    "#         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
    "\n",
    "# targets tensor([[ 9,  9,  3,  6,  7,  1,  0,  4,  0,  4,  0,  5,  0,  0,  0,  0],\n",
    "#         [ 7,  0,  6,  9,  9,  1,  5,  2,  3,  1,  8,  1,  5,  0,  0,  0],\n",
    "#         [ 1,  8,  1,  0,  5,  3,  2,  3,  0,  9,  3,  2,  4,  7,  0,  5],\n",
    "#         [ 9,  7,  9,  1,  0,  6,  2,  9, 11,  2,  4, 10, 11,  5,  0,  0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "nb_classes = 16\n",
    "seq_len = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " == logits == \n",
      " tensor([[[-0.0454, -1.6380,  2.3542,  ..., -0.9927,  0.5039, -1.6639],\n",
      "         [-2.0609, -0.1287,  1.5118,  ...,  1.4356, -0.2556,  0.1615],\n",
      "         [-0.5814,  0.3074,  0.1140,  ..., -0.5420,  0.2809, -0.6194],\n",
      "         ...,\n",
      "         [ 1.0790, -1.2793, -0.8957,  ...,  0.1858, -1.5915,  1.0700],\n",
      "         [ 0.3916, -0.0579, -0.3122,  ...,  0.7189,  0.1638, -1.3539],\n",
      "         [ 0.1964, -0.7673,  1.1060,  ...,  0.2558, -2.2480, -1.2574]],\n",
      "\n",
      "        [[-0.4934,  2.6456,  1.6421,  ...,  0.0258,  1.3745,  0.4510],\n",
      "         [ 1.0148, -0.2019,  1.1736,  ..., -0.9428,  0.6456,  0.2396],\n",
      "         [-0.8986, -0.0579, -0.8005,  ..., -0.8566, -1.2162,  1.1290],\n",
      "         ...,\n",
      "         [ 1.5684,  1.7940,  0.9422,  ..., -0.2551,  1.0192, -0.2893],\n",
      "         [ 0.3011,  0.8326,  1.9589,  ..., -0.0898, -0.1556,  0.6500],\n",
      "         [ 0.2842, -0.2804, -0.1563,  ..., -1.1728, -0.3659,  0.7056]],\n",
      "\n",
      "        [[ 1.7123,  0.0872, -1.6509,  ..., -0.7854, -1.4560, -1.4729],\n",
      "         [ 1.4867, -0.2596,  0.8423,  ...,  0.2700,  0.6697,  0.5177],\n",
      "         [ 0.8977, -0.9171, -1.3017,  ...,  0.4667,  0.1446,  0.2169],\n",
      "         ...,\n",
      "         [-0.4054,  0.3593,  1.3145,  ..., -0.4047,  1.5009,  0.5089],\n",
      "         [ 0.7142, -0.4150,  0.9000,  ...,  1.3147, -1.0631, -1.3368],\n",
      "         [ 0.1299, -0.1835, -1.7644,  ..., -0.6100, -0.1370, -0.6074]],\n",
      "\n",
      "        [[-0.4946, -1.4155,  0.7788,  ...,  1.0217,  0.5911, -0.4834],\n",
      "         [ 0.8206,  0.1340,  0.2019,  ..., -1.7021,  0.2336,  0.0573],\n",
      "         [-2.4922, -0.9691, -1.1457,  ...,  0.2568,  0.0953, -2.1060],\n",
      "         ...,\n",
      "         [-1.0476, -0.0850, -1.3696,  ...,  0.2892,  0.3711, -0.7373],\n",
      "         [ 0.2219, -0.1854, -0.6774,  ..., -0.4274, -1.3572,  0.0521],\n",
      "         [-0.8018, -0.7068,  0.0323,  ..., -0.0931, -0.0457, -1.3928]]])\n",
      "torch.Size([4, 16, 16])\n",
      " == targets == \n",
      " tensor([[15,  3,  7, 15, 14,  1, 14,  6,  2, 11, 10,  8,  3,  6, 14,  3],\n",
      "        [12,  5, 11, 14,  6, 15,  8,  1, 14,  5,  9,  1, 13, 14,  7,  4],\n",
      "        [ 4,  3, 11,  4, 12, 15,  1,  7,  1,  6,  1,  6, 15,  9,  1, 15],\n",
      "        [ 1, 13, 11,  8,  9, 12,  1,  5,  3, 11, 12, 10,  6, 10, 10, 11]])\n",
      "torch.Size([4, 16])\n",
      " == weights == \n",
      " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.randn(batch_size, seq_len, nb_classes)\n",
    "print(\" == logits == \\n\", logits)\n",
    "print(logits.shape)\n",
    "\n",
    "# don't have any 0s in targets as it effects weights of func 2.\n",
    "targets = torch.randint(1, 16, size=(batch_size, seq_len))\n",
    "#targets = torch.randn(batch_size, seq_len)\n",
    "print(\" == targets == \\n\", targets)\n",
    "print(targets.shape)\n",
    "\n",
    "weights = ([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
    "\n",
    "# weights = ([[1., 1., 1., 0.],\n",
    "#         [1., 1., 1., 0.]])\n",
    "\n",
    "weights = torch.FloatTensor(weights)\n",
    "print(\" == weights == \\n\", weights)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: original function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_cross_entropy_with_logits(\n",
    "    logits: torch.FloatTensor,\n",
    "    targets: torch.LongTensor,\n",
    "    weights: torch.FloatTensor,\n",
    "    average: str = \"batch\",\n",
    "    label_smoothing: float = None,\n",
    "    gamma: float = None,\n",
    "    alpha: Union[float, List[float], torch.FloatTensor] = None,\n",
    ") -> torch.FloatTensor:\n",
    "    \"\"\"\n",
    "    Computes the cross entropy loss of a sequence, weighted with respect to\n",
    "    some user provided weights. Note that the weighting here is not the same as\n",
    "    in the `torch.nn.CrossEntropyLoss()` criterion, which is weighting\n",
    "    classes; here we are weighting the loss contribution from particular elements\n",
    "    in the sequence. This allows loss computations for models which use padding.\n",
    "    # Parameters\n",
    "    logits : `torch.FloatTensor`, required.\n",
    "        A `torch.FloatTensor` of size (batch_size, sequence_length, num_classes)\n",
    "        which contains the unnormalized probability for each class.\n",
    "    targets : `torch.LongTensor`, required.\n",
    "        A `torch.LongTensor` of size (batch, sequence_length) which contains the\n",
    "        index of the true class for each corresponding step.\n",
    "    weights : `torch.FloatTensor`, required.\n",
    "        A `torch.FloatTensor` of size (batch, sequence_length)\n",
    "    average: str, optional (default = \"batch\")\n",
    "        If \"batch\", average the loss across the batches. If \"token\", average\n",
    "        the loss across each item in the input. If `None`, return a vector\n",
    "        of losses per batch element.\n",
    "    label_smoothing : `float`, optional (default = None)\n",
    "        Whether or not to apply label smoothing to the cross-entropy loss.\n",
    "        For example, with a label smoothing value of 0.2, a 4 class classification\n",
    "        target would look like `[0.05, 0.05, 0.85, 0.05]` if the 3rd class was\n",
    "        the correct label.\n",
    "    gamma : `float`, optional (default = None)\n",
    "        Focal loss[*] focusing parameter `gamma` to reduces the relative loss for\n",
    "        well-classified examples and put more focus on hard. The greater value\n",
    "        `gamma` is, the more focus on hard examples.\n",
    "    alpha : `float` or `List[float]`, optional (default = None)\n",
    "        Focal loss[*] weighting factor `alpha` to balance between classes. Can be\n",
    "        used independently with `gamma`. If a single `float` is provided, it\n",
    "        is assumed binary case using `alpha` and `1 - alpha` for positive and\n",
    "        negative respectively. If a list of `float` is provided, with the same\n",
    "        length as the number of classes, the weights will match the classes.\n",
    "        [*] T. Lin, P. Goyal, R. Girshick, K. He and P. Doll√°r, \"Focal Loss for\n",
    "        Dense Object Detection,\" 2017 IEEE International Conference on Computer\n",
    "        Vision (ICCV), Venice, 2017, pp. 2999-3007.\n",
    "    # Returns\n",
    "    A torch.FloatTensor representing the cross entropy loss.\n",
    "    If `average==\"batch\"` or `average==\"token\"`, the returned loss is a scalar.\n",
    "    If `average is None`, the returned loss is a vector of shape (batch_size,).\n",
    "    \"\"\"\n",
    "    if average not in {None, \"token\", \"batch\"}:\n",
    "        raise ValueError(\"Got average f{average}, expected one of None, 'token', or 'batch'\")\n",
    "\n",
    "    # make sure weights are float\n",
    "    weights = weights.float()\n",
    "    # sum all dim except batch\n",
    "    non_batch_dims = tuple(range(1, len(weights.shape)))\n",
    "    # shape : (batch_size,)\n",
    "    weights_batch_sum = weights.sum(dim=non_batch_dims)\n",
    "    # shape : (batch * sequence_length, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # shape : (batch * sequence_length, num_classes)\n",
    "    log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n",
    "    print(\"== log probs flat == \\n\", log_probs_flat)\n",
    "    print(log_probs_flat.shape)\n",
    "    \n",
    "    # shape : (batch * max_len, 1)\n",
    "    targets_flat = targets.view(-1, 1).long()\n",
    "    print(\"== targets flat == \\n\", targets_flat)\n",
    "    print(targets_flat.shape)\n",
    "    # focal loss coefficient\n",
    "    if gamma:\n",
    "        # shape : (batch * sequence_length, num_classes)\n",
    "        probs_flat = log_probs_flat.exp()\n",
    "        # shape : (batch * sequence_length,)\n",
    "        probs_flat = torch.gather(probs_flat, dim=1, index=targets_flat)\n",
    "        # shape : (batch * sequence_length,)\n",
    "        focal_factor = (1.0 - probs_flat) ** gamma\n",
    "        # shape : (batch, sequence_length)\n",
    "        focal_factor = focal_factor.view(*targets.size())\n",
    "        weights = weights * focal_factor\n",
    "\n",
    "    if alpha is not None:\n",
    "        # shape : () / (num_classes,)\n",
    "        if isinstance(alpha, (float, int)):\n",
    "\n",
    "            # shape : (2,)\n",
    "            alpha_factor = torch.tensor(\n",
    "                [1.0 - float(alpha), float(alpha)], dtype=weights.dtype, device=weights.device\n",
    "            )\n",
    "\n",
    "        elif isinstance(alpha, (list, numpy.ndarray, torch.Tensor)):\n",
    "\n",
    "            # shape : (c,)\n",
    "            alpha_factor = torch.tensor(alpha, dtype=weights.dtype, device=weights.device)\n",
    "\n",
    "            if not alpha_factor.size():\n",
    "                # shape : (1,)\n",
    "                alpha_factor = alpha_factor.view(1)\n",
    "                # shape : (2,)\n",
    "                alpha_factor = torch.cat([1 - alpha_factor, alpha_factor])\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                (\"alpha must be float, list of float, or torch.FloatTensor, {} provided.\").format(\n",
    "                    type(alpha)\n",
    "                )\n",
    "            )\n",
    "        # shape : (batch, max_len)\n",
    "        alpha_factor = torch.gather(alpha_factor, dim=0, index=targets_flat.view(-1)).view(\n",
    "            *targets.size()\n",
    "        )\n",
    "        weights = weights * alpha_factor\n",
    "\n",
    "    if label_smoothing is not None and label_smoothing > 0.0:\n",
    "        num_classes = logits.size(-1)\n",
    "        smoothing_value = label_smoothing / num_classes\n",
    "        # Fill all the correct indices with 1 - smoothing value.\n",
    "        one_hot_targets = torch.zeros_like(log_probs_flat).scatter_(\n",
    "            -1, targets_flat, 1.0 - label_smoothing\n",
    "        )\n",
    "        smoothed_targets = one_hot_targets + smoothing_value\n",
    "        negative_log_likelihood_flat = -log_probs_flat * smoothed_targets\n",
    "        negative_log_likelihood_flat = negative_log_likelihood_flat.sum(-1, keepdim=True)\n",
    "    else:\n",
    "        # Contribution to the negative log likelihood only comes from the exact indices\n",
    "        # of the targets, as the target distributions are one-hot. Here we use torch.gather\n",
    "        # to extract the indices of the num_classes dimension which contribute to the loss.\n",
    "        # shape : (batch * sequence_length, 1)\n",
    "        negative_log_likelihood_flat = -torch.gather(log_probs_flat, dim=1, index=targets_flat)\n",
    "        print(\"== negative_log_likelihood_flat == \\n\", negative_log_likelihood_flat)\n",
    "        print(negative_log_likelihood_flat.shape)\n",
    "    # shape : (batch, sequence_length)\n",
    "    negative_log_likelihood = negative_log_likelihood_flat.view(*targets.size())\n",
    "    print(\"== negative_log_likelihood == \\n\", negative_log_likelihood)\n",
    "    # shape : (batch, sequence_length)\n",
    "    negative_log_likelihood = negative_log_likelihood * weights\n",
    "    print(\"== negative_log_likelihood * weights == \\n\", negative_log_likelihood)\n",
    "\n",
    "    if average == \"batch\":\n",
    "        # shape : (batch_size,)\n",
    "        print(\"== nll summmed == \\n\", negative_log_likelihood.sum(non_batch_dims))\n",
    "        print(\"== weights_batch_sum == \\n\", weights_batch_sum)\n",
    "        per_batch_loss = negative_log_likelihood.sum(non_batch_dims) / (weights_batch_sum + 1e-13)\n",
    "        \n",
    "        num_non_empty_sequences = (weights_batch_sum > 0).float().sum() + 1e-13\n",
    "        print(\"== num_non_empty_sequences == \\n\", num_non_empty_sequences)\n",
    "        print(per_batch_loss.sum() / num_non_empty_sequences)\n",
    "        return per_batch_loss.sum() / num_non_empty_sequences\n",
    "    elif average == \"token\":\n",
    "        return negative_log_likelihood.sum() / (weights_batch_sum.sum() + 1e-13)\n",
    "    else:\n",
    "        # shape : (batch_size,)\n",
    "        per_batch_loss = negative_log_likelihood.sum(non_batch_dims) / (weights_batch_sum + 1e-13)\n",
    "        return per_batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== log probs flat == \n",
      " tensor([[-3.3966, -4.9893, -0.9970,  ..., -4.3439, -2.8473, -5.0151],\n",
      "        [-5.5061, -3.5739, -1.9334,  ..., -2.0096, -3.7008, -3.2837],\n",
      "        [-4.1109, -3.2221, -3.4155,  ..., -4.0715, -3.2486, -4.1489],\n",
      "        ...,\n",
      "        [-4.1386, -3.1759, -4.4606,  ..., -2.8018, -2.7199, -3.8283],\n",
      "        [-2.7388, -3.1461, -3.6381,  ..., -3.3881, -4.3178, -2.9086],\n",
      "        [-4.0282, -3.9332, -3.1941,  ..., -3.3195, -3.2721, -4.6192]])\n",
      "torch.Size([64, 16])\n",
      "== targets flat == \n",
      " tensor([[15],\n",
      "        [ 3],\n",
      "        [ 7],\n",
      "        [15],\n",
      "        [14],\n",
      "        [ 1],\n",
      "        [14],\n",
      "        [ 6],\n",
      "        [ 2],\n",
      "        [11],\n",
      "        [10],\n",
      "        [ 8],\n",
      "        [ 3],\n",
      "        [ 6],\n",
      "        [14],\n",
      "        [ 3],\n",
      "        [12],\n",
      "        [ 5],\n",
      "        [11],\n",
      "        [14],\n",
      "        [ 6],\n",
      "        [15],\n",
      "        [ 8],\n",
      "        [ 1],\n",
      "        [14],\n",
      "        [ 5],\n",
      "        [ 9],\n",
      "        [ 1],\n",
      "        [13],\n",
      "        [14],\n",
      "        [ 7],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 3],\n",
      "        [11],\n",
      "        [ 4],\n",
      "        [12],\n",
      "        [15],\n",
      "        [ 1],\n",
      "        [ 7],\n",
      "        [ 1],\n",
      "        [ 6],\n",
      "        [ 1],\n",
      "        [ 6],\n",
      "        [15],\n",
      "        [ 9],\n",
      "        [ 1],\n",
      "        [15],\n",
      "        [ 1],\n",
      "        [13],\n",
      "        [11],\n",
      "        [ 8],\n",
      "        [ 9],\n",
      "        [12],\n",
      "        [ 1],\n",
      "        [ 5],\n",
      "        [ 3],\n",
      "        [11],\n",
      "        [12],\n",
      "        [10],\n",
      "        [ 6],\n",
      "        [10],\n",
      "        [10],\n",
      "        [11]])\n",
      "torch.Size([64, 1])\n",
      "== negative_log_likelihood_flat == \n",
      " tensor([[5.0151],\n",
      "        [3.9453],\n",
      "        [2.1389],\n",
      "        [2.1164],\n",
      "        [2.7298],\n",
      "        [2.6152],\n",
      "        [1.8486],\n",
      "        [4.2873],\n",
      "        [3.1876],\n",
      "        [2.2365],\n",
      "        [1.6485],\n",
      "        [4.6421],\n",
      "        [3.7636],\n",
      "        [1.1032],\n",
      "        [2.5854],\n",
      "        [2.3322],\n",
      "        [3.4645],\n",
      "        [2.1004],\n",
      "        [3.2393],\n",
      "        [2.8127],\n",
      "        [3.4966],\n",
      "        [1.0997],\n",
      "        [2.6272],\n",
      "        [2.4631],\n",
      "        [3.2734],\n",
      "        [3.5114],\n",
      "        [3.5889],\n",
      "        [2.1161],\n",
      "        [1.0290],\n",
      "        [2.3270],\n",
      "        [2.7899],\n",
      "        [3.2665],\n",
      "        [4.2673],\n",
      "        [3.2289],\n",
      "        [3.5937],\n",
      "        [3.8497],\n",
      "        [5.4021],\n",
      "        [3.0619],\n",
      "        [1.9749],\n",
      "        [2.8571],\n",
      "        [3.6828],\n",
      "        [5.0188],\n",
      "        [2.9683],\n",
      "        [3.3676],\n",
      "        [2.3820],\n",
      "        [2.8993],\n",
      "        [3.6777],\n",
      "        [3.6905],\n",
      "        [4.5042],\n",
      "        [4.6783],\n",
      "        [3.0047],\n",
      "        [2.8324],\n",
      "        [3.5103],\n",
      "        [3.6239],\n",
      "        [4.1176],\n",
      "        [3.3697],\n",
      "        [2.9836],\n",
      "        [2.3515],\n",
      "        [2.2991],\n",
      "        [2.4307],\n",
      "        [2.7897],\n",
      "        [2.6591],\n",
      "        [2.5528],\n",
      "        [4.1979]])\n",
      "torch.Size([64, 1])\n",
      "== negative_log_likelihood == \n",
      " tensor([[5.0151, 3.9453, 2.1389, 2.1164, 2.7298, 2.6152, 1.8486, 4.2873, 3.1876,\n",
      "         2.2365, 1.6485, 4.6421, 3.7636, 1.1032, 2.5854, 2.3322],\n",
      "        [3.4645, 2.1004, 3.2393, 2.8127, 3.4966, 1.0997, 2.6272, 2.4631, 3.2734,\n",
      "         3.5114, 3.5889, 2.1161, 1.0290, 2.3270, 2.7899, 3.2665],\n",
      "        [4.2673, 3.2289, 3.5937, 3.8497, 5.4021, 3.0619, 1.9749, 2.8571, 3.6828,\n",
      "         5.0188, 2.9683, 3.3676, 2.3820, 2.8993, 3.6777, 3.6905],\n",
      "        [4.5042, 4.6783, 3.0047, 2.8324, 3.5103, 3.6239, 4.1176, 3.3697, 2.9836,\n",
      "         2.3515, 2.2991, 2.4307, 2.7897, 2.6591, 2.5528, 4.1979]])\n",
      "== negative_log_likelihood * weights == \n",
      " tensor([[5.0151, 3.9453, 2.1389, 2.1164, 2.7298, 2.6152, 1.8486, 4.2873, 3.1876,\n",
      "         2.2365, 1.6485, 4.6421, 3.7636, 0.0000, 0.0000, 0.0000],\n",
      "        [3.4645, 2.1004, 3.2393, 2.8127, 3.4966, 1.0997, 2.6272, 2.4631, 3.2734,\n",
      "         3.5114, 3.5889, 2.1161, 1.0290, 0.0000, 0.0000, 0.0000],\n",
      "        [4.2673, 3.2289, 3.5937, 3.8497, 5.4021, 3.0619, 1.9749, 2.8571, 3.6828,\n",
      "         5.0188, 2.9683, 3.3676, 2.3820, 2.8993, 0.0000, 0.0000],\n",
      "        [4.5042, 4.6783, 3.0047, 2.8324, 3.5103, 3.6239, 4.1176, 3.3697, 2.9836,\n",
      "         2.3515, 2.2991, 2.4307, 2.7897, 2.6591, 2.5528, 4.1979]])\n",
      "== nll summmed == \n",
      " tensor([40.1748, 34.8223, 48.5544, 51.9052])\n",
      "== weights_batch_sum == \n",
      " tensor([13., 13., 14., 16.])\n",
      "== num_non_empty_sequences == \n",
      " tensor(4.)\n",
      "tensor(3.1203)\n",
      "tensor(3.1203)\n"
     ]
    }
   ],
   "source": [
    "# get the loss \n",
    "loss = sequence_cross_entropy_with_logits(logits, targets, weights)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Case 2: my custom function\n",
    "\n",
    "def mask_tensor(tensor):\n",
    "    return (tensor != 0).float()\n",
    "\n",
    "def nested_sequence_cross_entropy_with_logits(\n",
    "    logits: torch.FloatTensor,\n",
    "    targets: torch.LongTensor,\n",
    "    weights: torch.FloatTensor,\n",
    "    average: str = \"batch\",\n",
    "    label_smoothing: float = None,\n",
    "    gamma: float = None,\n",
    "    alpha: Union[float, List[float], torch.FloatTensor] = None,\n",
    ") -> torch.FloatTensor:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    if average not in {None, \"token\", \"batch\"}:\n",
    "        raise ValueError(\"Got average f{average}, expected one of None, 'token', or 'batch'\")\n",
    "        \n",
    "    # weights mask, not sure if we need these this time..   \n",
    "    weights = weights.float()\n",
    "    print(\"weights\", weights)\n",
    "    # sum all dim except batch\n",
    "    non_batch_dims = tuple(range(1, len(weights.shape)))\n",
    "    \n",
    "    print(\"non batch dims\", non_batch_dims)\n",
    "    # shape : (batch_size,)\n",
    "    # sum weights along the row-dimension\n",
    "    # correspons to number of active elements for each item in the batch\n",
    "    weights_batch_sum = weights.sum(dim=non_batch_dims)\n",
    "    print(\"weights batch sum\", weights_batch_sum)\n",
    "    \n",
    "    \n",
    "    batch_size = targets.shape[0]    \n",
    "    seq_len = targets.shape[1]\n",
    "    nb_labels = targets.shape[-1]\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    #print(\"== logits flat == \\n\", logits_flat)\n",
    "    \n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n",
    "    print(\"== log probs flat == \\n\", log_probs_flat)\n",
    "    \n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    #target_flat = target.view(-1, 1)\n",
    "    targets_flat = targets.view(-1, nb_labels)\n",
    "    print(\"== targets flat == \\n\", targets_flat)\n",
    "    \n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    # dimensions need to be the same size except for the dim you are gathering on\n",
    "    # gathers from log probs the target index (row dim)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=targets_flat)\n",
    "    print(\"== losses flat == \\n\", losses_flat)\n",
    "    \n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*targets.size())\n",
    "    print(\"== losses == \\n\", losses)\n",
    "\n",
    "    # return the tensor where 1s correspond to active elements and 0s are padded elements\n",
    "    tensor_mask = mask_tensor(targets)\n",
    "    \n",
    "    # multiply losses by label mask\n",
    "    masked_losses = losses * tensor_mask\n",
    "    print(\"== masked losses == \\n\", masked_losses)\n",
    "    \n",
    "    # find the number of 0 elements in losses\n",
    "    # a 0 here means all elements were activate and n corresponds to how many zeros were in the row\n",
    "    num_zero_labels = (masked_losses == 0).sum(-1)\n",
    "    print(\"== num zero labels == \\n\", num_zero_labels)\n",
    "    \n",
    "    # possible number of values, multiply by constant\n",
    "    max_possible_labels = torch.ones(size=(batch_size, seq_len)) * nb_labels\n",
    "    \n",
    "    # subtract the number of possible active elements by the number of 0-padded elements\n",
    "    num_active_labels = max_possible_labels - num_zero_labels\n",
    "    print(\"== num active labels == \\n\", num_active_labels)\n",
    "  \n",
    "    # add loss along row-dim then divide by the number of active elements per row\n",
    "    per_batch_loss = masked_losses.sum(-1) / (num_active_labels + 1e-13)\n",
    "    print(\"== per_batch_loss == \\n\", per_batch_loss)\n",
    "    \n",
    "    # at this point, we are back to 2d shape as we have summed/divided by the number of labels\n",
    "    # we can use the regular weights values\n",
    "    # multiply losses by token-level mask\n",
    "    masked_per_batch_loss = per_batch_loss * weights\n",
    "    print(\"== masked_per_batch_loss == \\n\", masked_per_batch_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # shape : (batch_size,)\n",
    "    print(\"== masked_per_batch_loss_summmedd == \\n\", masked_per_batch_loss.sum(non_batch_dims))\n",
    "    # weights added up\n",
    "    print(\"== weights_batch_sum == \\n\", weights_batch_sum)\n",
    "    \n",
    "    per_batch_loss = masked_per_batch_loss.sum(non_batch_dims) / (weights_batch_sum + 1e-13)\n",
    "\n",
    "    num_non_empty_sequences = (weights_batch_sum > 0).float().sum() + 1e-13\n",
    "    print(\"== num_non_empty_sequences == \\n\", num_non_empty_sequences)\n",
    "    print(per_batch_loss.sum() / num_non_empty_sequences)\n",
    "    return per_batch_loss.sum() / num_non_empty_sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_targets(targets):\n",
    "    # need to add a 3rd dim and put the label there and pad it with 0s\n",
    "    expanded_targets = targets.unsqueeze(-1)\n",
    "    expanded_targets = expanded_targets.tolist()\n",
    "    \n",
    "    padding = [0,0]\n",
    "\n",
    "    for item in expanded_targets:\n",
    "        for labels in item:\n",
    "            labels.extend(padding)\n",
    "    \n",
    "    #print(expanded_targets)\n",
    "    expanded_targets = torch.LongTensor(expanded_targets)\n",
    "    return expanded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15,  3,  7, 15, 14,  1, 14,  6,  2, 11, 10,  8,  3,  6, 14,  3],\n",
      "        [12,  5, 11, 14,  6, 15,  8,  1, 14,  5,  9,  1, 13, 14,  7,  4],\n",
      "        [ 4,  3, 11,  4, 12, 15,  1,  7,  1,  6,  1,  6, 15,  9,  1, 15],\n",
      "        [ 1, 13, 11,  8,  9, 12,  1,  5,  3, 11, 12, 10,  6, 10, 10, 11]])\n",
      "torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "## Case 2\n",
    "print(targets)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_targets = expand_targets(targets)\n",
    "#print(expanded_targets)\n",
    "#expanded_targets = torch.LongTensor(expand_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "non batch dims (1,)\n",
      "weights batch sum tensor([13., 13., 14., 16.])\n",
      "== log probs flat == \n",
      " tensor([[-3.3966, -4.9893, -0.9970,  ..., -4.3439, -2.8473, -5.0151],\n",
      "        [-5.5061, -3.5739, -1.9334,  ..., -2.0096, -3.7008, -3.2837],\n",
      "        [-4.1109, -3.2221, -3.4155,  ..., -4.0715, -3.2486, -4.1489],\n",
      "        ...,\n",
      "        [-4.1386, -3.1759, -4.4606,  ..., -2.8018, -2.7199, -3.8283],\n",
      "        [-2.7388, -3.1461, -3.6381,  ..., -3.3881, -4.3178, -2.9086],\n",
      "        [-4.0282, -3.9332, -3.1941,  ..., -3.3195, -3.2721, -4.6192]])\n",
      "== targets flat == \n",
      " tensor([[15,  0,  0],\n",
      "        [ 3,  0,  0],\n",
      "        [ 7,  0,  0],\n",
      "        [15,  0,  0],\n",
      "        [14,  0,  0],\n",
      "        [ 1,  0,  0],\n",
      "        [14,  0,  0],\n",
      "        [ 6,  0,  0],\n",
      "        [ 2,  0,  0],\n",
      "        [11,  0,  0],\n",
      "        [10,  0,  0],\n",
      "        [ 8,  0,  0],\n",
      "        [ 3,  0,  0],\n",
      "        [ 6,  0,  0],\n",
      "        [14,  0,  0],\n",
      "        [ 3,  0,  0],\n",
      "        [12,  0,  0],\n",
      "        [ 5,  0,  0],\n",
      "        [11,  0,  0],\n",
      "        [14,  0,  0],\n",
      "        [ 6,  0,  0],\n",
      "        [15,  0,  0],\n",
      "        [ 8,  0,  0],\n",
      "        [ 1,  0,  0],\n",
      "        [14,  0,  0],\n",
      "        [ 5,  0,  0],\n",
      "        [ 9,  0,  0],\n",
      "        [ 1,  0,  0],\n",
      "        [13,  0,  0],\n",
      "        [14,  0,  0],\n",
      "        [ 7,  0,  0],\n",
      "        [ 4,  0,  0],\n",
      "        [ 4,  0,  0],\n",
      "        [ 3,  0,  0],\n",
      "        [11,  0,  0],\n",
      "        [ 4,  0,  0],\n",
      "        [12,  0,  0],\n",
      "        [15,  0,  0],\n",
      "        [ 1,  0,  0],\n",
      "        [ 7,  0,  0],\n",
      "        [ 1,  0,  0],\n",
      "        [ 6,  0,  0],\n",
      "        [ 1,  0,  0],\n",
      "        [ 6,  0,  0],\n",
      "        [15,  0,  0],\n",
      "        [ 9,  0,  0],\n",
      "        [ 1,  0,  0],\n",
      "        [15,  0,  0],\n",
      "        [ 1,  0,  0],\n",
      "        [13,  0,  0],\n",
      "        [11,  0,  0],\n",
      "        [ 8,  0,  0],\n",
      "        [ 9,  0,  0],\n",
      "        [12,  0,  0],\n",
      "        [ 1,  0,  0],\n",
      "        [ 5,  0,  0],\n",
      "        [ 3,  0,  0],\n",
      "        [11,  0,  0],\n",
      "        [12,  0,  0],\n",
      "        [10,  0,  0],\n",
      "        [ 6,  0,  0],\n",
      "        [10,  0,  0],\n",
      "        [10,  0,  0],\n",
      "        [11,  0,  0]])\n",
      "== losses flat == \n",
      " tensor([[5.0151, 3.3966, 3.3966],\n",
      "        [3.9453, 5.5061, 5.5061],\n",
      "        [2.1389, 4.1109, 4.1109],\n",
      "        [2.1164, 2.9072, 2.9072],\n",
      "        [2.7298, 2.2080, 2.2080],\n",
      "        [2.6152, 4.3759, 4.3759],\n",
      "        [1.8486, 2.4360, 2.4360],\n",
      "        [4.2873, 4.1995, 4.1995],\n",
      "        [3.1876, 3.1480, 3.1480],\n",
      "        [2.2365, 2.9949, 2.9949],\n",
      "        [1.6485, 2.7901, 2.7901],\n",
      "        [4.6421, 3.2056, 3.2056],\n",
      "        [3.7636, 3.8548, 3.8548],\n",
      "        [1.1032, 2.7994, 2.7994],\n",
      "        [2.5854, 2.3577, 2.3577],\n",
      "        [2.3322, 2.9042, 2.9042],\n",
      "        [3.4645, 4.4683, 4.4683],\n",
      "        [2.1004, 2.1024, 2.1024],\n",
      "        [3.2393, 3.9864, 3.9864],\n",
      "        [2.8127, 3.0143, 3.0143],\n",
      "        [3.4966, 3.2664, 3.2664],\n",
      "        [1.0997, 3.8935, 3.8935],\n",
      "        [2.6272, 4.2530, 4.2530],\n",
      "        [2.4631, 1.6757, 1.6757],\n",
      "        [3.2734, 2.3409, 2.3409],\n",
      "        [3.5114, 4.9083, 4.9083],\n",
      "        [3.5889, 2.6009, 2.6009],\n",
      "        [2.1161, 3.8496, 3.8496],\n",
      "        [1.0290, 4.1444, 4.1444],\n",
      "        [2.3270, 1.7778, 1.7778],\n",
      "        [2.7899, 3.3325, 3.3325],\n",
      "        [3.2665, 2.9352, 2.9352],\n",
      "        [4.2673, 1.3377, 1.3377],\n",
      "        [3.2289, 1.7078, 1.7078],\n",
      "        [3.5937, 2.3755, 2.3755],\n",
      "        [3.8497, 3.3595, 3.3595],\n",
      "        [5.4021, 1.0854, 1.0854],\n",
      "        [3.0619, 2.2555, 2.2555],\n",
      "        [1.9749, 2.4792, 2.4792],\n",
      "        [2.8571, 2.3168, 2.3168],\n",
      "        [3.6828, 2.9686, 2.9686],\n",
      "        [5.0188, 2.0907, 2.0907],\n",
      "        [2.9683, 4.3843, 4.3843],\n",
      "        [3.3676, 2.0036, 2.0036],\n",
      "        [2.3820, 3.8635, 3.8635],\n",
      "        [2.8993, 3.5967, 3.5967],\n",
      "        [3.6777, 2.5484, 2.5484],\n",
      "        [3.6905, 2.9533, 2.9533],\n",
      "        [4.5042, 3.5833, 3.5833],\n",
      "        [4.6783, 2.1556, 2.1556],\n",
      "        [3.0047, 5.2569, 5.2569],\n",
      "        [2.8324, 1.9563, 1.9563],\n",
      "        [3.5103, 3.1639, 3.1639],\n",
      "        [3.6239, 5.2994, 5.2994],\n",
      "        [4.1176, 1.6627, 1.6627],\n",
      "        [3.3697, 1.7078, 1.7078],\n",
      "        [2.9836, 5.3046, 5.3046],\n",
      "        [2.3515, 4.8343, 4.8343],\n",
      "        [2.2991, 2.5677, 2.5677],\n",
      "        [2.4307, 2.3749, 2.3749],\n",
      "        [2.7897, 3.9220, 3.9220],\n",
      "        [2.6591, 4.1386, 4.1386],\n",
      "        [2.5528, 2.7388, 2.7388],\n",
      "        [4.1979, 4.0282, 4.0282]])\n",
      "== losses == \n",
      " tensor([[[5.0151, 3.3966, 3.3966],\n",
      "         [3.9453, 5.5061, 5.5061],\n",
      "         [2.1389, 4.1109, 4.1109],\n",
      "         [2.1164, 2.9072, 2.9072],\n",
      "         [2.7298, 2.2080, 2.2080],\n",
      "         [2.6152, 4.3759, 4.3759],\n",
      "         [1.8486, 2.4360, 2.4360],\n",
      "         [4.2873, 4.1995, 4.1995],\n",
      "         [3.1876, 3.1480, 3.1480],\n",
      "         [2.2365, 2.9949, 2.9949],\n",
      "         [1.6485, 2.7901, 2.7901],\n",
      "         [4.6421, 3.2056, 3.2056],\n",
      "         [3.7636, 3.8548, 3.8548],\n",
      "         [1.1032, 2.7994, 2.7994],\n",
      "         [2.5854, 2.3577, 2.3577],\n",
      "         [2.3322, 2.9042, 2.9042]],\n",
      "\n",
      "        [[3.4645, 4.4683, 4.4683],\n",
      "         [2.1004, 2.1024, 2.1024],\n",
      "         [3.2393, 3.9864, 3.9864],\n",
      "         [2.8127, 3.0143, 3.0143],\n",
      "         [3.4966, 3.2664, 3.2664],\n",
      "         [1.0997, 3.8935, 3.8935],\n",
      "         [2.6272, 4.2530, 4.2530],\n",
      "         [2.4631, 1.6757, 1.6757],\n",
      "         [3.2734, 2.3409, 2.3409],\n",
      "         [3.5114, 4.9083, 4.9083],\n",
      "         [3.5889, 2.6009, 2.6009],\n",
      "         [2.1161, 3.8496, 3.8496],\n",
      "         [1.0290, 4.1444, 4.1444],\n",
      "         [2.3270, 1.7778, 1.7778],\n",
      "         [2.7899, 3.3325, 3.3325],\n",
      "         [3.2665, 2.9352, 2.9352]],\n",
      "\n",
      "        [[4.2673, 1.3377, 1.3377],\n",
      "         [3.2289, 1.7078, 1.7078],\n",
      "         [3.5937, 2.3755, 2.3755],\n",
      "         [3.8497, 3.3595, 3.3595],\n",
      "         [5.4021, 1.0854, 1.0854],\n",
      "         [3.0619, 2.2555, 2.2555],\n",
      "         [1.9749, 2.4792, 2.4792],\n",
      "         [2.8571, 2.3168, 2.3168],\n",
      "         [3.6828, 2.9686, 2.9686],\n",
      "         [5.0188, 2.0907, 2.0907],\n",
      "         [2.9683, 4.3843, 4.3843],\n",
      "         [3.3676, 2.0036, 2.0036],\n",
      "         [2.3820, 3.8635, 3.8635],\n",
      "         [2.8993, 3.5967, 3.5967],\n",
      "         [3.6777, 2.5484, 2.5484],\n",
      "         [3.6905, 2.9533, 2.9533]],\n",
      "\n",
      "        [[4.5042, 3.5833, 3.5833],\n",
      "         [4.6783, 2.1556, 2.1556],\n",
      "         [3.0047, 5.2569, 5.2569],\n",
      "         [2.8324, 1.9563, 1.9563],\n",
      "         [3.5103, 3.1639, 3.1639],\n",
      "         [3.6239, 5.2994, 5.2994],\n",
      "         [4.1176, 1.6627, 1.6627],\n",
      "         [3.3697, 1.7078, 1.7078],\n",
      "         [2.9836, 5.3046, 5.3046],\n",
      "         [2.3515, 4.8343, 4.8343],\n",
      "         [2.2991, 2.5677, 2.5677],\n",
      "         [2.4307, 2.3749, 2.3749],\n",
      "         [2.7897, 3.9220, 3.9220],\n",
      "         [2.6591, 4.1386, 4.1386],\n",
      "         [2.5528, 2.7388, 2.7388],\n",
      "         [4.1979, 4.0282, 4.0282]]])\n",
      "== masked losses == \n",
      " tensor([[[5.0151, 0.0000, 0.0000],\n",
      "         [3.9453, 0.0000, 0.0000],\n",
      "         [2.1389, 0.0000, 0.0000],\n",
      "         [2.1164, 0.0000, 0.0000],\n",
      "         [2.7298, 0.0000, 0.0000],\n",
      "         [2.6152, 0.0000, 0.0000],\n",
      "         [1.8486, 0.0000, 0.0000],\n",
      "         [4.2873, 0.0000, 0.0000],\n",
      "         [3.1876, 0.0000, 0.0000],\n",
      "         [2.2365, 0.0000, 0.0000],\n",
      "         [1.6485, 0.0000, 0.0000],\n",
      "         [4.6421, 0.0000, 0.0000],\n",
      "         [3.7636, 0.0000, 0.0000],\n",
      "         [1.1032, 0.0000, 0.0000],\n",
      "         [2.5854, 0.0000, 0.0000],\n",
      "         [2.3322, 0.0000, 0.0000]],\n",
      "\n",
      "        [[3.4645, 0.0000, 0.0000],\n",
      "         [2.1004, 0.0000, 0.0000],\n",
      "         [3.2393, 0.0000, 0.0000],\n",
      "         [2.8127, 0.0000, 0.0000],\n",
      "         [3.4966, 0.0000, 0.0000],\n",
      "         [1.0997, 0.0000, 0.0000],\n",
      "         [2.6272, 0.0000, 0.0000],\n",
      "         [2.4631, 0.0000, 0.0000],\n",
      "         [3.2734, 0.0000, 0.0000],\n",
      "         [3.5114, 0.0000, 0.0000],\n",
      "         [3.5889, 0.0000, 0.0000],\n",
      "         [2.1161, 0.0000, 0.0000],\n",
      "         [1.0290, 0.0000, 0.0000],\n",
      "         [2.3270, 0.0000, 0.0000],\n",
      "         [2.7899, 0.0000, 0.0000],\n",
      "         [3.2665, 0.0000, 0.0000]],\n",
      "\n",
      "        [[4.2673, 0.0000, 0.0000],\n",
      "         [3.2289, 0.0000, 0.0000],\n",
      "         [3.5937, 0.0000, 0.0000],\n",
      "         [3.8497, 0.0000, 0.0000],\n",
      "         [5.4021, 0.0000, 0.0000],\n",
      "         [3.0619, 0.0000, 0.0000],\n",
      "         [1.9749, 0.0000, 0.0000],\n",
      "         [2.8571, 0.0000, 0.0000],\n",
      "         [3.6828, 0.0000, 0.0000],\n",
      "         [5.0188, 0.0000, 0.0000],\n",
      "         [2.9683, 0.0000, 0.0000],\n",
      "         [3.3676, 0.0000, 0.0000],\n",
      "         [2.3820, 0.0000, 0.0000],\n",
      "         [2.8993, 0.0000, 0.0000],\n",
      "         [3.6777, 0.0000, 0.0000],\n",
      "         [3.6905, 0.0000, 0.0000]],\n",
      "\n",
      "        [[4.5042, 0.0000, 0.0000],\n",
      "         [4.6783, 0.0000, 0.0000],\n",
      "         [3.0047, 0.0000, 0.0000],\n",
      "         [2.8324, 0.0000, 0.0000],\n",
      "         [3.5103, 0.0000, 0.0000],\n",
      "         [3.6239, 0.0000, 0.0000],\n",
      "         [4.1176, 0.0000, 0.0000],\n",
      "         [3.3697, 0.0000, 0.0000],\n",
      "         [2.9836, 0.0000, 0.0000],\n",
      "         [2.3515, 0.0000, 0.0000],\n",
      "         [2.2991, 0.0000, 0.0000],\n",
      "         [2.4307, 0.0000, 0.0000],\n",
      "         [2.7897, 0.0000, 0.0000],\n",
      "         [2.6591, 0.0000, 0.0000],\n",
      "         [2.5528, 0.0000, 0.0000],\n",
      "         [4.1979, 0.0000, 0.0000]]])\n",
      "== num zero labels == \n",
      " tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]])\n",
      "== num active labels == \n",
      " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "== per_batch_loss == \n",
      " tensor([[5.0151, 3.9453, 2.1389, 2.1164, 2.7298, 2.6152, 1.8486, 4.2873, 3.1876,\n",
      "         2.2365, 1.6485, 4.6421, 3.7636, 1.1032, 2.5854, 2.3322],\n",
      "        [3.4645, 2.1004, 3.2393, 2.8127, 3.4966, 1.0997, 2.6272, 2.4631, 3.2734,\n",
      "         3.5114, 3.5889, 2.1161, 1.0290, 2.3270, 2.7899, 3.2665],\n",
      "        [4.2673, 3.2289, 3.5937, 3.8497, 5.4021, 3.0619, 1.9749, 2.8571, 3.6828,\n",
      "         5.0188, 2.9683, 3.3676, 2.3820, 2.8993, 3.6777, 3.6905],\n",
      "        [4.5042, 4.6783, 3.0047, 2.8324, 3.5103, 3.6239, 4.1176, 3.3697, 2.9836,\n",
      "         2.3515, 2.2991, 2.4307, 2.7897, 2.6591, 2.5528, 4.1979]])\n",
      "== masked_per_batch_loss == \n",
      " tensor([[5.0151, 3.9453, 2.1389, 2.1164, 2.7298, 2.6152, 1.8486, 4.2873, 3.1876,\n",
      "         2.2365, 1.6485, 4.6421, 3.7636, 0.0000, 0.0000, 0.0000],\n",
      "        [3.4645, 2.1004, 3.2393, 2.8127, 3.4966, 1.0997, 2.6272, 2.4631, 3.2734,\n",
      "         3.5114, 3.5889, 2.1161, 1.0290, 0.0000, 0.0000, 0.0000],\n",
      "        [4.2673, 3.2289, 3.5937, 3.8497, 5.4021, 3.0619, 1.9749, 2.8571, 3.6828,\n",
      "         5.0188, 2.9683, 3.3676, 2.3820, 2.8993, 0.0000, 0.0000],\n",
      "        [4.5042, 4.6783, 3.0047, 2.8324, 3.5103, 3.6239, 4.1176, 3.3697, 2.9836,\n",
      "         2.3515, 2.2991, 2.4307, 2.7897, 2.6591, 2.5528, 4.1979]])\n",
      "== masked_per_batch_loss_summmedd == \n",
      " tensor([40.1748, 34.8223, 48.5544, 51.9052])\n",
      "== weights_batch_sum == \n",
      " tensor([13., 13., 14., 16.])\n",
      "== num_non_empty_sequences == \n",
      " tensor(4.)\n",
      "tensor(3.1203)\n"
     ]
    }
   ],
   "source": [
    "loss2 = nested_sequence_cross_entropy_with_logits(logits, expanded_targets, weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
